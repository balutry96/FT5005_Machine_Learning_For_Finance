{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# reference：　https://www.kaggle.com/rajmehra03/topic-modelling-using-lda-and-lsa-in-sklearn\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "# configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "nltk.download('punkt') # nltk needs various resources\n",
    "nltk.download('wordnet') # nltk needs various resources\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "# text data preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df=pd.read_csv('abcnews-date-text.csv')\n",
    "# drop the publish date.\n",
    "df.drop(['publish_date'],axis=1,inplace=True)\n",
    "df.head() # take a look at the first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>headline_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>decides community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>fire witness must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>staff aust strike rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>strike affect australian traveller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  \\\n",
       "0  aba decides against community broadcasting lic...   \n",
       "1     act fire witnesses must be aware of defamation   \n",
       "2     a g calls for infrastructure protection summit   \n",
       "3           air nz staff in aust strike for pay rise   \n",
       "4      air nz strike to affect australian travellers   \n",
       "\n",
       "                    headline_cleaned_text  \n",
       "0  decides community broadcasting licence  \n",
       "1      fire witness must aware defamation  \n",
       "2   call infrastructure protection summit  \n",
       "3                  staff aust strike rise  \n",
       "4      strike affect australian traveller  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(headline): # you can add more preprocessing functions here\n",
    "  le=WordNetLemmatizer()\n",
    "  word_tokens=word_tokenize(headline)\n",
    "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "  cleaned_text=\" \".join(tokens)\n",
    "  return cleaned_text\n",
    "\n",
    "# apply the clean_text function and create a new column to save results.\n",
    "df['headline_cleaned_text']=df['headline_text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF vector, apply stop words, keep only 1000 features (1000 columns)\n",
    "vect =TfidfVectorizer(stop_words=list(stop_words),max_features=1000) \n",
    "# 1000 selected by term frequency across the corpus.\n",
    "# do note that high TF does not mean that word must be more informative and useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on our input df, only one column to create TF-IDF vector\n",
    "vect_text=vect.fit_transform(df['headline_cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226258, 1000)\n",
      "  (0, 507)\t0.7830964759517771\n",
      "  (0, 180)\t0.6219002406752289\n",
      "  (1, 575)\t0.6350011874790689\n",
      "  (1, 982)\t0.634252507913514\n",
      "  (1, 322)\t0.44101842150367176\n",
      "  (2, 850)\t0.6547003749683041\n",
      "  (2, 681)\t0.6236747415657183\n",
      "  (2, 124)\t0.42707989387150563\n",
      "  (3, 743)\t0.4535400720072263\n",
      "  (3, 842)\t0.4901743730039168\n",
      "  (3, 56)\t0.5225164976545785\n",
      "  (3, 826)\t0.5301009307789318\n",
      "  (4, 58)\t0.6373967041659779\n",
      "  (4, 842)\t0.7705358145591605\n",
      "  (5, 977)\t1.0\n",
      "  (6, 709)\t1.0\n",
      "  (7, 542)\t0.5180441448099484\n",
      "  (7, 345)\t0.48078322560362474\n",
      "  (7, 960)\t0.535924910123952\n",
      "  (7, 55)\t0.46180325325287314\n",
      "  (8, 452)\t0.42931170443114947\n",
      "  (8, 202)\t0.34965626131197547\n",
      "  (8, 775)\t0.4483609681844147\n",
      "  (8, 13)\t0.5219542868074372\n",
      "  (8, 56)\t0.4690075948807512\n",
      "  :\t:\n",
      "  (1226249, 831)\t0.3778999040452349\n",
      "  (1226250, 100)\t0.5663193054263862\n",
      "  (1226250, 941)\t0.503939080598977\n",
      "  (1226250, 548)\t0.4622873009035714\n",
      "  (1226250, 135)\t0.4600198895370982\n",
      "  (1226251, 344)\t0.6080525617065659\n",
      "  (1226251, 969)\t0.7938967704948061\n",
      "  (1226252, 500)\t0.6238436340607845\n",
      "  (1226252, 169)\t0.6216900029959888\n",
      "  (1226252, 57)\t0.4736250208938309\n",
      "  (1226253, 994)\t0.5066405611890687\n",
      "  (1226253, 524)\t0.7105693787979459\n",
      "  (1226253, 63)\t0.48826888050819595\n",
      "  (1226254, 207)\t0.7720691816264106\n",
      "  (1226254, 813)\t0.63553849512262\n",
      "  (1226255, 197)\t0.4709048526948187\n",
      "  (1226255, 941)\t0.5131899033214004\n",
      "  (1226255, 994)\t0.43571949255955134\n",
      "  (1226255, 734)\t0.570116888577325\n",
      "  (1226256, 207)\t0.5078702413212534\n",
      "  (1226256, 511)\t0.5626250693467827\n",
      "  (1226256, 508)\t0.4574039151076636\n",
      "  (1226256, 253)\t0.4650833342176444\n",
      "  (1226257, 127)\t0.7598601186999014\n",
      "  (1226257, 984)\t0.65008660962165\n"
     ]
    }
   ],
   "source": [
    "# check the dimension and content of vect_text\n",
    "print(vect_text.shape)\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to extract idf\n",
    "idf=vect.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['police', 'say', 'plan', 'call', 'council', 'australia', 'court', 'fire', 'govt', 'interview', 'back', 'woman', 'australian', 'death', 'crash', 'sydney', 'water', 'year', 'report', 'face', 'health', 'attack', 'change', 'charged', 'child', 'take', 'home', 'murder', 'world', 'claim', 'hospital', 'market', 'government', 'school', 'election', 'charge', 'coast', 'south', 'open', 'drug', 'help', 'farmer', 'china', 'found', 'minister', 'first', 'accused', 'killed', 'national', 'service', 'case', 'group', 'worker', 'road', 'boost', 'north', 'melbourne', 'talk', 'coronavirus', 'rise', 'fear', 'missing', 'final', 'price', 'urged', 'test', 'trial', 'dy', 'labor', 'power', 'win', 'green', 'queensland', 'business', 'family', 'record', 'rural', 'driver', 'funding', 'house', 'deal', 'indigenous', 'centre', 'make', 'high', 'lead', 'rate', 'gold', 'show', 'work', 'could', 'fight', 'return', 'dead', 'time', 'adelaide', 'state', 'concern', 'seek', 'flood', 'push', 'mine', 'union', 'probe', 'public', 'victim', 'former', 'support', 'want', 'budget', 'news', 'fund', 'community', 'west', 'canberra', 'hope', 'resident', 'industry', 'search', 'country', 'still', 'find', 'iraq', 'brisbane', 'perth', 'leader', 'inquiry', 'protest', 'urge', 'guilty', 'move', 'aussie', 'jailed', 'student', 'opposition', 'get', 'trump', 'appeal', 'life', 'assault', 'three', 'teen', 'jail', 'share', 'review', 'storm', 'kill', 'force', 'park', 'warning', 'port', 'fall', 'mayor', 'rain', 'local', 'strike', 'defends', 'sale', 'future', 'farm', 'chief', 'action', 'federal', 'reject', 'body', 'doctor', 'close', 'victoria', 'begin', 'drought', 'head', 'arrested', 'island', 'law', 'job', 'land', 'second', 'study', 'hour', 'safety', 'offer', 'mining', 'tasmania', 'ahead', 'cut', 'welcome', 'threat', 'continues', 'hold', 'fatal', 'abuse', 'security', 'decision', 'cost', 'station', 'weather', 'regional', 'shooting', 'bank', 'party', 'vote', 'river', 'four', 'season', 'bushfire', 'start', 'tour', 'game', 'bill', 'darwin', 'housing', 'hit', 'campaign', 'injured', 'hill', 'alleged', 'denies', 'spark', 'warns', 'blaze', 'people', 'company', 'keep', 'play', 'question', 'rule', 'sign', 'india', 'look', 'liberal', 'premier', 'told', 'delay', 'train', 'east', 'rail', 'medium', 'target', 'project', 'live', 'city', 'give', 'need', 'violence', 'beat', 'officer', 'anti', 'club', 'join', 'england', 'meet', 'risk', 'star', 'trade', 'million', 'tiger', 'break', 'tourism', 'free', 'tasmanian', 'league', 'team', 'care', 'near', 'climate', 'pakistan', 'blue', 'black', 'speaks', 'president', 'baby', 'food', 'wont', 'plane', 'clash', 'week', 'launch', 'stop', 'murray', 'teacher', 'right', 'crime', 'award', 'demand', 'hunter', 'scheme', 'town', 'challenge', 'japan', 'battle', 'chinese', 'debate', 'arrest', 'coal', 'despite', 'continue', 'race', 'title', 'airport', 'release', 'stand', 'blast', 'young', 'cancer', 'loss', 'program', 'site', 'light', 'emergency', 'meeting', 'royal', 'policy', 'rudd', 'aust', 'crisis', 'cattle', 'issue', 'accident', 'girl', 'shark', 'victorian', 'john', 'korea', 'boat', 'parliament', 'bomb', 'prison', 'number', 'beach', 'investigation', 'victory', 'five', 'research', 'cyclone', 'injury', 'promise', 'asylum', 'youth', 'hobart', 'grower', 'investigate', 'pacific', 'plant', 'highway', 'development', 'street', 'defence', 'bail', 'shire', 'stay', 'abbott', 'staff', 'export', 'soldier', 'toll', 'expert', 'shot', 'rescue', 'bush', 'stabbing', 'highlight', 'legal', 'mark', 'indonesia', 'commission', 'good', 'christmas', 'pleads', 'visit', 'donald', 'truck', 'expected', 'central', 'hunt', 'aboriginal', 'player', 'inquest', 'wind', 'northern', 'ready', 'festival', 'troop', 'drop', 'rally', 'track', 'profit', 'coach', 'dollar', 'sport', 'long', 'building', 'announces', 'discus', 'considers', 'turn', 'thousand', 'control', 'newcastle', 'damage', 'prompt', 'western', 'nuclear', 'lawyer', 'warned', 'post', 'protester', 'amid', 'super', 'system', 'tourist', 'covid', 'focus', 'increase', 'line', 'mental', 'mother', 'save', 'authority', 'lake', 'impact', 'energy', 'fuel', 'cause', 'double', 'drum', 'tell', 'name', 'match', 'flight', 'education', 'fined', 'british', 'raid', 'bos', 'front', 'early', 'problem', 'closure', 'pressure', 'michael', 'smith', 'poll', 'candidate', 'sentence', 'hears', 'scott', 'david', 'howard', 'leave', 'another', 'miner', 'owner', 'interest', 'doubt', 'scientist', 'suicide', 'leaf', 'slam', 'pledge', 'closer', 'last', 'dairy', 'patient', 'named', 'point', 'third', 'wild', 'restriction', 'behind', 'crop', 'king', 'fiji', 'iran', 'clean', 'international', 'miss', 'seeker', 'raise', 'refugee', 'training', 'better', 'link', 'strong', 'firefighter', 'wall', 'peter', 'rape', 'money', 'tree', 'official', 'step', 'outback', 'senate', 'iraqi', 'crew', 'reach', 'condition', 'result', 'reform', 'friday', 'alice', 'loses', 'march', 'cricket', 'lose', 'property', 'put', 'dispute', 'released', 'suspect', 'swan', 'upgrade', 'hotel', 'shortage', 'facing', 'broken', 'nurse', 'french', 'escape', 'coalition', 'carbon', 'rock', 'great', 'turnbull', 'parent', 'wallaby', 'reveals', 'go', 'border', 'month', 'white', 'hand', 'andrew', 'driving', 'honour', 'draw', 'ship', 'extended', 'military', 'political', 'russia', 'aged', 'horse', 'indonesian', 'confident', 'indian', 'rugby', 'firm', 'video', 'foreign', 'obama', 'consider', 'next', 'growth', 'killer', 'come', 'southern', 'round', 'killing', 'remains', 'planning', 'grandstand', 'stolen', 'united', 'level', 'comment', 'israel', 'bird', 'contract', 'grain', 'illegal', 'end', 'drink', 'mill', 'needed', 'fine', 'grand', 'qantas', 'father', 'waste', 'rebel', 'bali', 'best', 'detention', 'drive', 'economic', 'bomber', 'alcohol', 'judge', 'survey', 'film', 'wine', 'admits', 'afghan', 'fishing', 'economy', 'olympic', 'order', 'hearing', 'global', 'woe', 'councillor', 'safe', 'animal', 'stage', 'major', 'speed', 'phone', 'board', 'france', 'terrorism', 'response', 'effort', 'human', 'sentenced', 'monday', '2015', 'church', 'kid', 'sell', 'james', 'domestic', 'term', 'hurt', 'stock', 'evidence', 'bridge', 'medical', 'mount', 'bull', 'transport', 'figure', 'ash', 'grant', 'pair', 'history', 'russian', 'wednesday', 'lift', 'away', 'spot', 'lion', 'tuesday', 'collapse', 'list', 'sexual', 'financial', 'benefit', 'witness', 'worry', 'office', 'valley', 'sought', 'supply', 'protection', 'must', 'story', 'likely', 'theft', 'blame', 'dog', 'expansion', 'fish', 'telstra', 'robbery', 'spring', 'capital', 'disease', 'full', 'part', 'tough', 'army', 'merger', 'nation', 'pilot', 'fails', 'tackle', 'threatens', 'gillard', 'fraud', 'thursday', 'bendigo', 'set', 'crackdown', 'surgery', 'conference', 'prisoner', 'seven', 'host', 'planned', '2014', 'zealand', 'role', 'clear', 'region', 'suspected', 'online', 'treatment', 'syria', 'forum', 'london', 'creek', 'whale', 'afghanistan', 'higher', 'leadership', 'suspended', 'teenager', 'critical', 'given', 'disaster', 'origin', 'burn', 'bombing', 'quake', 'philippine', 'thai', 'social', 'flag', 'check', 'hundred', 'ban', 'member', 'beef', 'eagle', 'seat', 'rescued', 'held', 'williams', 'camp', 'coroner', 'squad', 'travel', 'forest', 'wife', 'real', 'series', 'anzac', 'heat', 'confirms', 'territory', 'watch', 'sheep', 'peace', 'kangaroo', 'university', 'relief', 'agreement', 'blamed', 'solar', 'update', 'lost', 'crowd', 'debt', 'lower', 'couple', 'harvest', 'finance', 'cash', 'across', 'quits', 'access', 'heritage', 'chase', 'music', 'giant', 'tony', 'warrior', 'researcher', 'investment', 'townsville', 'cross', 'lanka', 'see', 'remain', 'corruption', 'flooding', 'africa', 'wrap', 'night', 'address', 'jones', 'detail', 'cabinet', 'chance', 'volunteer', 'survivor', 'brown', 'dump', 'plea', 'possible', 'welfare', 'late', 'recovery', 'cleared', 'management', 'billion', 'knight', 'reef', 'caught', 'mackay', 'made', 'remote', 'latest', 'vow', 'petrol', 'socceroos', 'heart', 'politics', 'book', 'alert', '2016', 'racing', 'outbreak', 'armed', 'day', 'well', 'israeli', 'memorial', 'prince', 'tsunami', 'fruit', 'zone', 'half', 'timor', 'historic', 'general', 'kimberley', 'rising', 'resource', 'george', 'marine', 'winner', 'quarter', 'reporter', 'takeover', 'concerned', 'tribute', 'industrial', 'gippsland', 'keen', 'veteran', 'without', 'approval', 'area', 'director', 'japanese', 'semi', 'spill', 'anger', 'wait', 'operation', 'revamp', 'nine', 'looking', 'praise', 'commissioner', 'confidence', 'bashing', 'card', 'linked', 'deadly', 'small', 'marriage', 'allegation', 'champion', 'short', 'used', 'bushfires', 'gain', 'senator', 'fan', 'journalist', 'august', 'would', 'bulldog', 'resume', 'sugar', 'johnson', 'rare', 'aim', 'form', 'gaza', 'morrison', 'passenger', 'september', 'boom', 'brother', 'queen', 'fisherman', 'brawl', 'proposed', 'tipped', 'palestinian', 'paul', 'custody', 'incident', 'sector', 'data', 'success', 'private', 'weekend', 'taxi', 'trading', 'pull', 'surge', 'working', 'pool', 'producer', 'space', 'extra', 'terror', 'unit', 'warn', 'explosion', 'place', 'ambulance', 'toddler', 'july', 'proposal', 'finding', 'harbour', 'left', 'roll', 'bronco', 'analysis', 'blow', 'edge', 'loom', 'zimbabwe', 'june', 'production', 'banned', 'native', 'ease', 'happy', 'prepare', 'solomon', 'charity', 'traffic', 'disability', 'breach', 'making', 'underway', 'construction', 'limit', 'reserve', 'cairn', 'leak', 'committee', 'press', 'hawk', 'riot', 'red', 'cannabis', 'fighting', 'uranium', 'elderly', 'darling', 'licence', 'testing', 'wood', 'bikie', 'penalty', 'struggle', 'brings', 'asbestos', 'like', 'mixed', 'clarke', 'crow', 'april', 'hall', 'milk', 'holiday', 'celebrates', 'event', 'forced', 'islamic', 'asked', 'positive', 'factory', 'spending', 'option', 'artist', 'fresh', 'october', 'summit', 'protect', 'senior', 'compensation', 'activist', 'november', 'scandal', '2013', 'date', 'forecast', 'iron', 'prize', 'walk']\n",
      "police walk\n",
      "4.440524277043323\n",
      "7.922757250789557\n"
     ]
    }
   ],
   "source": [
    "dd=dict(zip(vect.get_feature_names_out(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "print(l) # print out sorted output\n",
    "print(l[0],l[-1]) # too many, so we check only the most common and least common words by idf\n",
    "print(dd['police']) # we know police is the most common from the previous line's output, print idf\n",
    "print(dd['walk'])  # police is the most common and walk is least common among the news headlines, print idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226258, 10)\n",
      "[[0.0415801  0.0415801  0.0415801  ... 0.0415801  0.0415801  0.0415801 ]\n",
      " [0.03689667 0.49289935 0.21189715 ... 0.03689967 0.03689847 0.03689695]\n",
      " [0.0369732  0.03696543 0.03697667 ... 0.03696401 0.03696574 0.38155244]\n",
      " ...\n",
      " [0.54332106 0.0334604  0.03345428 ... 0.03345415 0.03345461 0.18906484]\n",
      " [0.35235181 0.03341149 0.03341149 ... 0.03341252 0.03341202 0.22139288]\n",
      " [0.04151237 0.0415052  0.62647723 ... 0.04150434 0.04150474 0.04149814]]\n",
      "--- 735.5526669025421 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Now we finally reach the step for LDA. This step takes long.\n",
    "# In the original reference, it covers LSA too. Before LDA, LSA was the state-of-art for document clustering. \n",
    "# In recent years, few people use lSA and most people use LDA. So you can focus on LDA first.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# the following is the key function for the basic LDA\n",
    "lda_model=LatentDirichletAllocation(n_components=10,random_state=88,n_jobs=3) # n_jobs could be important now because LDA is slow\n",
    "# parameters:\n",
    "# n_components is the number of topics\n",
    "# doc_topic_priorfloat, default=None => you can specify the prior probability of topics\n",
    "# topic_word_priorfloat, default=None => you can specify the prior probability of words\n",
    "# learning_method{‘batch’, ‘online’}, default=’batch’: if the data size is large, the online update will be much faster than the batch update.\n",
    "\n",
    "# the training step that takes long time\n",
    "lda_top=lda_model.fit_transform(vect_text)\n",
    "\n",
    "print(lda_top.shape)  # (no_of_doc,no_of_topics)\n",
    "print(lda_top)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# also remember that this is only for one value of no. of topics and you may need to try several times to tune the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# check whether the proportion adds up to 1\n",
    "sum=0\n",
    "for i in lda_top[0]:\n",
    "  sum=sum+i\n",
    "print(sum)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: \n",
      "Topic  0 :  4.158009834670271 %\n",
      "Topic  1 :  4.158009834687848 %\n",
      "Topic  2 :  4.15800983466907 %\n",
      "Topic  3 :  4.1580098346681185 %\n",
      "Topic  4 :  4.158009834671056 %\n",
      "Topic  5 :  4.1580099586966295 %\n",
      "Topic  6 :  62.5779113639259 %\n",
      "Topic  7 :  4.158009834672468 %\n",
      "Topic  8 :  4.158009834669851 %\n",
      "Topic  9 :  4.158009834668804 %\n"
     ]
    }
   ],
   "source": [
    "# composition of doc 0 for eg\n",
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.49925056e-01 8.98117340e-01 1.00006301e-01 ... 1.00005695e-01\n",
      "  1.00003637e-01 1.00004235e-01]\n",
      " [1.00003536e-01 1.00001617e-01 1.00001913e-01 ... 1.00004652e-01\n",
      "  1.00003116e-01 1.00004240e-01]\n",
      " [1.00002405e-01 1.00002166e-01 1.00003376e-01 ... 1.00005018e-01\n",
      "  1.00003599e-01 1.00004349e-01]\n",
      " ...\n",
      " [1.00002182e-01 1.00002816e-01 1.00003197e-01 ... 1.00010387e-01\n",
      "  1.00004223e-01 1.00002854e-01]\n",
      " [1.00002668e-01 1.00002094e-01 1.00003054e-01 ... 1.00004496e-01\n",
      "  1.00002431e-01 1.00006169e-01]\n",
      " [1.00003285e-01 1.00002185e-01 1.00003830e-01 ... 1.00004737e-01\n",
      "  8.51341721e+02 1.00004178e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "# print the probaility over words for each topic\n",
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)  # (no_of_topics*no_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "hospital health case make child leader state mayor doctor warning \n",
      "\n",
      "Topic 1: \n",
      "crash dy killed police north road flood still iraq driver \n",
      "\n",
      "Topic 2: \n",
      "water charged final show south canberra trial green australia drought \n",
      "\n",
      "Topic 3: \n",
      "court face charge report accused life defends jail indigenous find \n",
      "\n",
      "Topic 4: \n",
      "china talk rural fire work budget mine country national trump \n",
      "\n",
      "Topic 5: \n",
      "council open school market urged rate first want centre aussie \n",
      "\n",
      "Topic 6: \n",
      "interview world home coast gold community guilty back second shooting \n",
      "\n",
      "Topic 7: \n",
      "sydney brisbane move queensland storm future assault perth protest denies \n",
      "\n",
      "Topic 8: \n",
      "missing take change search industry family appeal found sale park \n",
      "\n",
      "Topic 9: \n",
      "election win lead jailed say law year australian land action \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most important words (top 10) for each topic\n",
    "vocab = vect.get_feature_names_out()\n",
    "\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10] # this is where you change the top 10 to topX\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# The reference also have few more blocks of codes of Word Cloud.\n",
    "# But that package wordcloud is not supported for python version 3.7 & above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -14374029.07542995\n",
      "--- 88.814706325531 seconds ---\n",
      "Perplexity:  1912.1684872552237\n",
      "--- 86.81644320487976 seconds ---\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'batch',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': 3,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 88,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# now we output goodness-of-fit measures\n",
    "from pprint import pprint\n",
    "\n",
    "# Log Likelyhood: Higher is better\n",
    "start_time = time.time()\n",
    "print(\"Log Likelihood: \", lda_model.score(vect_text))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "start_time = time.time()\n",
    "print(\"Perplexity: \", lda_model.perplexity(vect_text))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
